{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import langdetect\n",
    "from functools import lru_cache\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "DetectorFactory.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('../../src'))\n",
    "\n",
    "from helper_functions.path_resolver import DynamicPathResolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\phishing_bert\n"
     ]
    }
   ],
   "source": [
    "dpr = DynamicPathResolver(marker=\"README.md\")\n",
    "\n",
    "data_mail_dir = dpr.path.data.raw.data_mail.sets._path\n",
    "\n",
    "train_paths_curated = [\n",
    "    dpr.path.data.raw.data_mail.curated.CEAS_08_csv,\n",
    "    dpr.path.data.raw.data_mail.curated.TREC_07_csv\n",
    "]\n",
    "\n",
    "test_paths_curated = [\n",
    "    dpr.path.data.raw.data_mail.curated.Nazario_5_csv,\n",
    "    dpr.path.data.raw.data_mail.curated.SpamAssasin_csv\n",
    "]\n",
    "\n",
    "paths_own = [\n",
    "    dpr.path.data.raw.data_mail.own.mails_combined_csv,\n",
    "   # dpr.path.data.raw.data_mail.own.mails_jannis.jannis_mail_csv\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = GoogleTranslator(source=\"en\", target=\"de\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return langdetect.detect(str(text))\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "    \n",
    "\n",
    "def translate_to_de(text):\n",
    "    try:\n",
    "        return translator.translate(text)\n",
    "    except Exception:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_balanced_set(base_file, out_file, total_size, language=\"en\"):\n",
    "    per_class = total_size // 2\n",
    "\n",
    "    df = pd.read_csv(base_file)\n",
    "    df = df[df[\"label\"].isin([0, 1]) & (df[\"language\"] == language)]\n",
    "\n",
    "    legit_samp = df[df[\"label\"] == 0].sample(n=per_class, random_state=42)\n",
    "    phish_samp = df[df[\"label\"] == 1].sample(n=per_class, random_state=42)\n",
    "\n",
    "    balanced = pd.concat([legit_samp, phish_samp], ignore_index=True)\n",
    "    balanced.fillna({\"subject\": \"\", \"body\": \"\"}, inplace=True)\n",
    "    balanced.to_csv(out_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_entire_dataset(eng_file, out_file):\n",
    "    df = pd.read_csv(eng_file)\n",
    "    print(f\"Translating dataset: {eng_file}\")\n",
    "    \n",
    "    df['subject'] = [translate_to_de(text) for text in tqdm(df['subject'], desc=\"Translating subjects\")]\n",
    "    df['body'] = [translate_to_de(text) for text in tqdm(df['body'], desc=\"Translating bodies\")]\n",
    "    df['language'] = \"de\"\n",
    "    df.to_csv(out_file, index=False)\n",
    "\n",
    "    print(f\"Translated dataset saved: {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_existing_datasets(eng_file, de_file, out_file, english_size, german_size):\n",
    "    for file, desc in zip([eng_file, de_file], [\"English\", \"German\"]):\n",
    "        tqdm.write(f\"Processing {desc} file: {file}\")\n",
    "        \n",
    "    df_eng = pd.read_csv(eng_file)\n",
    "    df_de = pd.read_csv(de_file)\n",
    "\n",
    "    # Split English into phishing (1) and legit (0)\n",
    "    eng_phishing = df_eng[df_eng[\"label\"] == 1]\n",
    "    eng_legit = df_eng[df_eng[\"label\"] == 0]\n",
    "\n",
    "    # Split German into phishing (1) and legit (0)\n",
    "    de_phishing = df_de[df_de[\"label\"] == 1]\n",
    "    de_legit = df_de[df_de[\"label\"] == 0]\n",
    "\n",
    "    # First half phishing, second half legit\n",
    "    eng_phishing_sample = eng_phishing.iloc[:english_size // 2]  \n",
    "    eng_legit_sample = eng_legit.iloc[:english_size // 2]\n",
    "\n",
    "    de_phishing_sample = de_phishing.iloc[-german_size // 2:]  \n",
    "    de_legit_sample = de_legit.iloc[-german_size // 2:]\n",
    "\n",
    "    # Combine\n",
    "    combined = pd.concat([eng_phishing_sample, eng_legit_sample, de_phishing_sample, de_legit_sample], ignore_index=True)\n",
    "    combined.to_csv(out_file, index=False)\n",
    "\n",
    "    print(f\"Built multilingual dataset: {out_file}\")\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(df, name):\n",
    "    print(f\"\\n{name}, Rows: {len(df)}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    class_counts = df[\"label\"].value_counts().to_dict()\n",
    "    lang_counts = df[\"language\"].value_counts().to_dict()\n",
    "    grouped = df.groupby([\"label\", \"language\"]).size().to_dict()\n",
    "\n",
    "    print(f\"Class Distribution: {class_counts}\")\n",
    "    print(f\"Language Distribution: {lang_counts}\")\n",
    "    print(f\"Detailed (Class, Language) Distribution: {grouped}\")\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lang_and_create_base(file_paths, out_file):\n",
    "    dfs = []\n",
    "    for path in tqdm(file_paths, desc=\"Preprocessing and combining files\", unit=\"file\"):\n",
    "        df = pd.read_csv(path)\n",
    "        df[\"language\"] = df[\"subject\"].progress_apply(detect_language)\n",
    "        dfs.append(df)\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    combined.to_csv(out_file, index=False)\n",
    "    print(f\"Combined preprocessed file saved: {out_file}\")\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def balance_and_split_dataset(base_file, train_file, test_file, test_size=0.2, random_state=42):\n",
    "    df = pd.read_csv(base_file)\n",
    "    df = df[df[\"label\"].isin([0, 1])]\n",
    "    \n",
    "    class_counts = df[\"label\"].value_counts()\n",
    "    print(\"Original dataset distribution:\\n\", class_counts)\n",
    "\n",
    "    min_class_size = min(class_counts)\n",
    "    legit_sample = df[df[\"label\"] == 0]  \n",
    "    phish_sample = df[df[\"label\"] == 1].sample(n=min_class_size, random_state=random_state)  \n",
    "\n",
    "    df_balanced = pd.concat([legit_sample, phish_sample], ignore_index=True)\n",
    "    print(\"Balanced dataset distribution:\\n\", df_balanced[\"label\"].value_counts())\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_balanced, \n",
    "        test_size=test_size, \n",
    "        stratify=df_balanced[\"label\"], \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    train_df.to_csv(train_file, index=False)\n",
    "    test_df.to_csv(test_file, index=False)\n",
    "\n",
    "    print(f\"Train set saved: {train_file}, Size: {len(train_df)}\")\n",
    "    print(f\"Test set saved: {test_file}, Size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language adding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_train_base = os.path.join(data_mail_dir, \"mails_combined.csv\")\n",
    "curated_test_base  = os.path.join(data_mail_dir, \"curated_test_base.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add language col to bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_lang_and_create_base(train_paths_curated, curated_train_base)\n",
    "add_lang_and_create_base(test_paths_curated, curated_test_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curated_train_base = pd.read_csv(curated_train_base)\n",
    "verify(df_curated_train_base, \"Curated Train Base\")\n",
    "\n",
    "df_curated_test_base = pd.read_csv(curated_test_base)\n",
    "verify(df_curated_test_base, \"Curated Test Base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Curated Train & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. BERT – English Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_balanced_set(\n",
    "    base_file=curated_train_base,\n",
    "    out_file=os.path.join(data_mail_dir, \"english_curated_train.csv\"),\n",
    "    total_size=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_balanced_set(\n",
    "    base_file=curated_test_base,\n",
    "    out_file=os.path.join(data_mail_dir, \"english_curated_test.csv\"),\n",
    "    total_size=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. BERT – German Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_entire_dataset(\n",
    "    eng_file=os.path.join(data_mail_dir, \"english_curated_verification.csv\"),\n",
    "    out_file=os.path.join(data_mail_dir, \"german_curated_verification.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_entire_dataset(\n",
    "    eng_file=os.path.join(data_mail_dir, \"english_curated_train.csv\"),\n",
    "    out_file=os.path.join(data_mail_dir, \"german_curated_train.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_entire_dataset(\n",
    "    eng_file=os.path.join(data_mail_dir, \"english_curated_test.csv\"),\n",
    "    out_file=os.path.join(data_mail_dir, \"german_curated_test.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. BERT – Multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_existing_datasets(\n",
    "    eng_file=os.path.join(data_mail_dir, \"english_curated_verification.csv\"),\n",
    "    de_file=os.path.join(data_mail_dir, \"german_curated_verification.csv\"),\n",
    "    out_file=os.path.join(data_mail_dir, \"multilingual_curated_verification.csv\"),\n",
    "    english_size=2000,\n",
    "    german_size=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_existing_datasets(\n",
    "    eng_file=os.path.join(data_mail_dir, \"english_curated_train.csv\"),\n",
    "    de_file=os.path.join(data_mail_dir, \"german_curated_train.csv\"),\n",
    "    out_file=os.path.join(data_mail_dir, \"multilingual_curated_train.csv\"),\n",
    "    english_size=10000,\n",
    "    german_size=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_existing_datasets(\n",
    "    eng_file=os.path.join(data_mail_dir, \"english_curated_test.csv\"),\n",
    "    de_file=os.path.join(data_mail_dir, \"german_curated_test.csv\"),\n",
    "    out_file=os.path.join(data_mail_dir, \"multilingual_curated_test.csv\"),\n",
    "    english_size=2000,\n",
    "    german_size=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample non overlapping (Verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_non_overlapping(train_curated_paths, used_train_file, output_file, sample_size=4000):\n",
    "    # Load and concatenate datasets\n",
    "    df_list = [pd.read_csv(path) for path in train_curated_paths]\n",
    "    full_data = pd.concat(df_list, ignore_index=True)\n",
    "    used_data = pd.read_csv(used_train_file)\n",
    "\n",
    "    # Drop duplicates\n",
    "    full_data = full_data.drop_duplicates(subset=[\"subject\", \"body\"])\n",
    "    used_data = used_data.drop_duplicates(subset=[\"subject\", \"body\"])\n",
    "\n",
    "    # Detect language\n",
    "    full_data[\"language\"] = full_data[\"body\"].apply(detect_language)\n",
    "\n",
    "    # Keep only English samples\n",
    "    full_data = full_data[full_data[\"language\"] == \"en\"]\n",
    "\n",
    "    # Merge with used data to find non-overlapping samples\n",
    "    merged = full_data.merge(\n",
    "        used_data,\n",
    "        on=[\"subject\", \"body\"],\n",
    "        how=\"left\",\n",
    "        indicator=True,\n",
    "        suffixes=(\"\", \"_drop\")\n",
    "    )\n",
    "\n",
    "    new_data = (\n",
    "        merged\n",
    "        .query(\"_merge == 'left_only'\")\n",
    "        .drop(columns=[\"_merge\"] + [col for col in merged.columns if col.endswith(\"_drop\")])\n",
    "    )\n",
    "\n",
    "    # Sample a balanced dataset\n",
    "    per_class = sample_size // 2\n",
    "    legit_samples = new_data[new_data[\"label\"] == 0].sample(n=per_class, random_state=42)\n",
    "    phish_samples = new_data[new_data[\"label\"] == 1].sample(n=per_class, random_state=42)\n",
    "    balanced_sample = pd.concat([legit_samples, phish_samples], ignore_index=True)\n",
    "\n",
    "    # Save\n",
    "    balanced_sample.to_csv(output_file, index=False)\n",
    "\n",
    "    # Plot class distribution\n",
    "    class_dist = balanced_sample[\"label\"].value_counts()\n",
    "    print(\"\\nClass Distribution:\\n\", class_dist)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=class_dist.index, y=class_dist.values)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    return balanced_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_train_file = os.path.join(data_mail_dir, \"english_curated_train.csv\")\n",
    "output_file = os.path.join(data_mail_dir, \"english_curated_verification.csv\")\n",
    "\n",
    "sample_non_overlapping(train_paths_curated, used_train_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(csv_path1, csv_path2):\n",
    "    df1 = pd.read_csv(csv_path1)\n",
    "    df2 = pd.read_csv(csv_path2)\n",
    "\n",
    "    combined_df = pd.concat([df1, df2])\n",
    "    duplicates = combined_df.duplicated(keep=False)\n",
    "    num_duplicates = duplicates.sum()\n",
    "\n",
    "    print(f\"Number of duplicate rows across both CSVs: {num_duplicates}\")\n",
    "    return num_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file1 = dpr.path.data.raw.data_mail.sets.english_curated_test_csv \n",
    "csv_file2 = dpr.path.data.raw.data_mail.sets.english_curated_verification_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicates = check_duplicates(csv_file1, csv_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_base = os.path.join(data_mail_dir, \"own_base.csv\")\n",
    "own_train_base = os.path.join(data_mail_dir, \"own_train_base.csv\")\n",
    "own_test_base  = os.path.join(data_mail_dir, \"own_test_base.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715f3be3f07246b3ba1b9ed2f9073910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing and combining files:   0%|          | 0/1 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf8e92a7912438f813a1cc9e13e2606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined preprocessed file saved: c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\phishing_bert\\data\\raw\\data_mail\\sets\\own_base.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ilian\\\\Documents\\\\Projects\\\\git_projects\\\\university\\\\phishing_bert\\\\data\\\\raw\\\\data_mail\\\\sets\\\\own_base.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_lang_and_create_base(paths_own, own_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset distribution:\n",
      " label\n",
      "1    4543\n",
      "0    3609\n",
      "Name: count, dtype: int64\n",
      "Balanced dataset distribution:\n",
      " label\n",
      "0    3609\n",
      "1    3609\n",
      "Name: count, dtype: int64\n",
      "Train set saved: c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\phishing_bert\\data\\raw\\data_mail\\sets\\own_train_base.csv, Size: 5774\n",
      "Test set saved: c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\phishing_bert\\data\\raw\\data_mail\\sets\\own_test_base.csv, Size: 1444\n"
     ]
    }
   ],
   "source": [
    "balance_and_split_dataset(own_base, own_train_base, own_test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Own Train Base, Rows: 5774\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 2887, 1: 2887}\n",
      "Language Distribution: {'de': 4981, 'en': 352, 'ru': 220, 'af': 28, 'sv': 19, 'fr': 18, 'unknown': 16, 'nl': 15, 'no': 15, 'et': 13, 'tl': 12, 'it': 10, 'mk': 10, 'id': 10, 'da': 10, 'bg': 8, 'tr': 6, 'cy': 6, 'pl': 5, 'es': 3, 'uk': 3, 'ca': 3, 'ro': 2, 'hu': 2, 'so': 2, 'fi': 2, 'ja': 1, 'pt': 1, 'lv': 1}\n",
      "Detailed (Class, Language) Distribution: {(0, 'af'): 18, (0, 'ca'): 1, (0, 'cy'): 1, (0, 'da'): 9, (0, 'de'): 2528, (0, 'en'): 264, (0, 'et'): 13, (0, 'fi'): 1, (0, 'fr'): 12, (0, 'hu'): 2, (0, 'id'): 4, (0, 'it'): 2, (0, 'nl'): 4, (0, 'no'): 7, (0, 'ro'): 1, (0, 'sv'): 9, (0, 'tl'): 9, (0, 'tr'): 1, (0, 'unknown'): 1, (1, 'af'): 10, (1, 'bg'): 8, (1, 'ca'): 2, (1, 'cy'): 5, (1, 'da'): 1, (1, 'de'): 2453, (1, 'en'): 88, (1, 'es'): 3, (1, 'fi'): 1, (1, 'fr'): 6, (1, 'id'): 6, (1, 'it'): 8, (1, 'ja'): 1, (1, 'lv'): 1, (1, 'mk'): 10, (1, 'nl'): 11, (1, 'no'): 8, (1, 'pl'): 5, (1, 'pt'): 1, (1, 'ro'): 1, (1, 'ru'): 220, (1, 'so'): 2, (1, 'sv'): 10, (1, 'tl'): 3, (1, 'tr'): 5, (1, 'uk'): 3, (1, 'unknown'): 15}\n",
      "----------------------------------------\n",
      "\n",
      "Own Test Base, Rows: 1444\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 722, 1: 722}\n",
      "Language Distribution: {'de': 1267, 'en': 83, 'ru': 45, 'af': 7, 'nl': 5, 'et': 4, 'sv': 4, 'mk': 4, 'pl': 3, 'fr': 3, 'tl': 3, 'da': 3, 'no': 3, 'it': 2, 'id': 2, 'unknown': 1, 'bg': 1, 'uk': 1, 'sk': 1, 'es': 1, 'so': 1}\n",
      "Detailed (Class, Language) Distribution: {(0, 'af'): 5, (0, 'da'): 1, (0, 'de'): 641, (0, 'en'): 59, (0, 'et'): 4, (0, 'fr'): 2, (0, 'id'): 1, (0, 'nl'): 1, (0, 'no'): 1, (0, 'pl'): 1, (0, 'sk'): 1, (0, 'sv'): 2, (0, 'tl'): 3, (1, 'af'): 2, (1, 'bg'): 1, (1, 'da'): 2, (1, 'de'): 626, (1, 'en'): 24, (1, 'es'): 1, (1, 'fr'): 1, (1, 'id'): 1, (1, 'it'): 2, (1, 'mk'): 4, (1, 'nl'): 4, (1, 'no'): 2, (1, 'pl'): 2, (1, 'ru'): 45, (1, 'so'): 1, (1, 'sv'): 2, (1, 'uk'): 1, (1, 'unknown'): 1}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_own_train_base = pd.read_csv(own_train_base)\n",
    "verify(df_own_train_base, \"Own Train Base\")\n",
    "\n",
    "df_own_test_base = pd.read_csv(own_test_base)\n",
    "verify(df_own_test_base, \"Own Test Base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Balances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multilangual_curated_verification = pd.read_csv(os.path.join(data_mail_dir, \"english_curated_verification.csv\"))\n",
    "verify(df_multilangual_curated_verification, \"English Curated Verification\")\n",
    "\n",
    "df_multilangual_curated_verification = pd.read_csv(os.path.join(data_mail_dir, \"german_curated_verification.csv\"))\n",
    "verify(df_multilangual_curated_verification, \"German Curated Verification\")\n",
    "\n",
    "df_multilangual_curated_verification = pd.read_csv(os.path.join(data_mail_dir, \"multilingual_curated_verification.csv\"))\n",
    "verify(df_multilangual_curated_verification, \"Multilingual Curated Verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English Curated Train, Rows: 20000\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 10000, 1: 10000}\n",
      "Language Distribution: {'en': 20000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'en'): 10000, (1, 'en'): 10000}\n",
      "----------------------------------------\n",
      "\n",
      "English Curated Test, Rows: 4000\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 2000, 1: 2000}\n",
      "Language Distribution: {'en': 4000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'en'): 2000, (1, 'en'): 2000}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Curated-based train english\n",
    "df_eng_curated_train = pd.read_csv(os.path.join(data_mail_dir, \"english_curated_train.csv\"))\n",
    "verify(df_eng_curated_train, \"English Curated Train\")\n",
    "\n",
    "# Curated-based test english\n",
    "df_eng_curated_test = pd.read_csv(os.path.join(data_mail_dir, \"english_curated_test.csv\"))\n",
    "verify(df_eng_curated_test, \"English Curated Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "German Curated Train, Rows: 20000\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 10000, 1: 10000}\n",
      "Language Distribution: {'de': 20000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'de'): 10000, (1, 'de'): 10000}\n",
      "----------------------------------------\n",
      "\n",
      "German Curated Test, Rows: 4000\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 2000, 1: 2000}\n",
      "Language Distribution: {'de': 4000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'de'): 2000, (1, 'de'): 2000}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Curated-based train german\n",
    "df_germ_curated_train = pd.read_csv(os.path.join(data_mail_dir, \"german_curated_train.csv\"))\n",
    "verify(df_germ_curated_train, \"German Curated Train\")\n",
    "\n",
    "# Curated-based test german\n",
    "df_germ_curated_test = pd.read_csv(os.path.join(data_mail_dir, \"german_curated_test.csv\"))\n",
    "verify(df_germ_curated_test, \"German Curated Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multilingual Curated Train, Rows: 20000\n",
      "----------------------------------------\n",
      "Class Distribution: {1: 10000, 0: 10000}\n",
      "Language Distribution: {'en': 10000, 'de': 10000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'de'): 5000, (0, 'en'): 5000, (1, 'de'): 5000, (1, 'en'): 5000}\n",
      "----------------------------------------\n",
      "\n",
      "Multilingual Curated Test, Rows: 4000\n",
      "----------------------------------------\n",
      "Class Distribution: {1: 2000, 0: 2000}\n",
      "Language Distribution: {'en': 2000, 'de': 2000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'de'): 1000, (0, 'en'): 1000, (1, 'de'): 1000, (1, 'en'): 1000}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Curated-based train multi\n",
    "df_mult_curated_train = pd.read_csv(os.path.join(data_mail_dir, \"multilingual_curated_train.csv\"))\n",
    "verify(df_mult_curated_train, \"Multilingual Curated Train\")\n",
    "\n",
    "# Curated-based test multi\n",
    "df_mult_curated_test = pd.read_csv(os.path.join(data_mail_dir, \"multilingual_curated_test.csv\"))\n",
    "verify(df_mult_curated_test, \"Multilingual Curated Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Own Base, Rows: 8162\n",
      "----------------------------------------\n",
      "Class Distribution: {1: 4543, 0: 3609, -1: 10}\n",
      "Language Distribution: {'de': 7047, 'en': 470, 'ru': 335, 'af': 42, 'sv': 26, 'fr': 24, 'nl': 23, 'no': 21, 'unknown': 21, 'et': 18, 'bg': 16, 'tl': 15, 'it': 15, 'mk': 14, 'da': 13, 'id': 13, 'pl': 10, 'tr': 7, 'cy': 6, 'uk': 4, 'ca': 4, 'es': 4, 'so': 3, 'hu': 2, 'ro': 2, 'fi': 2, 'pt': 1, 'lv': 1, 'hr': 1, 'ja': 1, 'sk': 1}\n",
      "Detailed (Class, Language) Distribution: {(-1, 'de'): 8, (-1, 'en'): 1, (-1, 'no'): 1, (0, 'af'): 23, (0, 'ca'): 1, (0, 'cy'): 1, (0, 'da'): 10, (0, 'de'): 3169, (0, 'en'): 323, (0, 'et'): 17, (0, 'fi'): 1, (0, 'fr'): 14, (0, 'hu'): 2, (0, 'id'): 5, (0, 'it'): 2, (0, 'nl'): 5, (0, 'no'): 8, (0, 'pl'): 1, (0, 'ro'): 1, (0, 'sk'): 1, (0, 'sv'): 11, (0, 'tl'): 12, (0, 'tr'): 1, (0, 'unknown'): 1, (1, 'af'): 19, (1, 'bg'): 16, (1, 'ca'): 3, (1, 'cy'): 5, (1, 'da'): 3, (1, 'de'): 3870, (1, 'en'): 146, (1, 'es'): 4, (1, 'et'): 1, (1, 'fi'): 1, (1, 'fr'): 10, (1, 'hr'): 1, (1, 'id'): 8, (1, 'it'): 13, (1, 'ja'): 1, (1, 'lv'): 1, (1, 'mk'): 14, (1, 'nl'): 18, (1, 'no'): 12, (1, 'pl'): 9, (1, 'pt'): 1, (1, 'ro'): 1, (1, 'ru'): 335, (1, 'so'): 3, (1, 'sv'): 15, (1, 'tl'): 3, (1, 'tr'): 6, (1, 'uk'): 4, (1, 'unknown'): 20}\n",
      "----------------------------------------\n",
      "\n",
      "German Own Train, Rows: 5774\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 2887, 1: 2887}\n",
      "Language Distribution: {'de': 4981, 'en': 352, 'ru': 220, 'af': 28, 'sv': 19, 'fr': 18, 'unknown': 16, 'nl': 15, 'no': 15, 'et': 13, 'tl': 12, 'it': 10, 'mk': 10, 'id': 10, 'da': 10, 'bg': 8, 'tr': 6, 'cy': 6, 'pl': 5, 'es': 3, 'uk': 3, 'ca': 3, 'ro': 2, 'hu': 2, 'so': 2, 'fi': 2, 'ja': 1, 'pt': 1, 'lv': 1}\n",
      "Detailed (Class, Language) Distribution: {(0, 'af'): 18, (0, 'ca'): 1, (0, 'cy'): 1, (0, 'da'): 9, (0, 'de'): 2528, (0, 'en'): 264, (0, 'et'): 13, (0, 'fi'): 1, (0, 'fr'): 12, (0, 'hu'): 2, (0, 'id'): 4, (0, 'it'): 2, (0, 'nl'): 4, (0, 'no'): 7, (0, 'ro'): 1, (0, 'sv'): 9, (0, 'tl'): 9, (0, 'tr'): 1, (0, 'unknown'): 1, (1, 'af'): 10, (1, 'bg'): 8, (1, 'ca'): 2, (1, 'cy'): 5, (1, 'da'): 1, (1, 'de'): 2453, (1, 'en'): 88, (1, 'es'): 3, (1, 'fi'): 1, (1, 'fr'): 6, (1, 'id'): 6, (1, 'it'): 8, (1, 'ja'): 1, (1, 'lv'): 1, (1, 'mk'): 10, (1, 'nl'): 11, (1, 'no'): 8, (1, 'pl'): 5, (1, 'pt'): 1, (1, 'ro'): 1, (1, 'ru'): 220, (1, 'so'): 2, (1, 'sv'): 10, (1, 'tl'): 3, (1, 'tr'): 5, (1, 'uk'): 3, (1, 'unknown'): 15}\n",
      "----------------------------------------\n",
      "\n",
      "German Own Test, Rows: 1444\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 722, 1: 722}\n",
      "Language Distribution: {'de': 1267, 'en': 83, 'ru': 45, 'af': 7, 'nl': 5, 'et': 4, 'sv': 4, 'mk': 4, 'pl': 3, 'fr': 3, 'tl': 3, 'da': 3, 'no': 3, 'it': 2, 'id': 2, 'unknown': 1, 'bg': 1, 'uk': 1, 'sk': 1, 'es': 1, 'so': 1}\n",
      "Detailed (Class, Language) Distribution: {(0, 'af'): 5, (0, 'da'): 1, (0, 'de'): 641, (0, 'en'): 59, (0, 'et'): 4, (0, 'fr'): 2, (0, 'id'): 1, (0, 'nl'): 1, (0, 'no'): 1, (0, 'pl'): 1, (0, 'sk'): 1, (0, 'sv'): 2, (0, 'tl'): 3, (1, 'af'): 2, (1, 'bg'): 1, (1, 'da'): 2, (1, 'de'): 626, (1, 'en'): 24, (1, 'es'): 1, (1, 'fr'): 1, (1, 'id'): 1, (1, 'it'): 2, (1, 'mk'): 4, (1, 'nl'): 4, (1, 'no'): 2, (1, 'pl'): 2, (1, 'ru'): 45, (1, 'so'): 1, (1, 'sv'): 2, (1, 'uk'): 1, (1, 'unknown'): 1}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Own Base \n",
    "df_own_base = pd.read_csv(os.path.join(data_mail_dir, \"own_base.csv\"))\n",
    "verify(df_own_base, \"Own Base\")\n",
    "\n",
    "# Own Train\n",
    "df_own_train_base = pd.read_csv(os.path.join(data_mail_dir, \"own_train_base.csv\"))\n",
    "verify(df_own_train_base, \"German Own Train\")\n",
    "\n",
    "# Own Test\n",
    "df_own_test_base = pd.read_csv(os.path.join(data_mail_dir, \"own_test_base.csv\"))\n",
    "verify(df_own_test_base, \"German Own Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
